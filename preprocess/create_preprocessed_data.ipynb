{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create_preprocessed_data.ipynb\n",
    "This notebook contains code to lightly preprocess and organize the NETSS data (../gtarc.csv) and HL7 data (../MVPS/gen_core.csv) into folders such that the folder names represent jurisdictions and files within represent disease state codes. For example, NETSS data for varicella in Florida would be located under ./NETSS/Florida/10030.csv. The purpose is to make the synthetic data generation more efficient; instead of loading and processing the entire raw dataset each time, one can leverage the folder/file structure created from this script to load a ready-made subset directly.\n",
    "\n",
    "The preprocessing performed is as follows:\n",
    "\n",
    "NETSS Data:\n",
    "1. Discard records where COUNT is not equal to 1, per CDC recommendations.\n",
    "2. Discard records having a missing EVENTD.\n",
    "3. Only retain records with complete years: Currently, 2014-2019, but generally 2014 through the year that this script is being run, minus 1.\n",
    "4. For each STATE and EVENT type, create a zero-padded time series such that the series begins on the first day of the first month of the first event and ends on the last day of the last month of the last event.\n",
    "\n",
    "HL7 Data (Gen Core):\n",
    "1. Define a constant COUNT variable equal to 1 (to align with the NETSS 1 row = 1 COUNT methodology). This implies that 1 row = 1 record in the HL7 data.\n",
    "2. Per a CSELS/GTRI meeting on 10/19/2020, subset to `current_record_flag == 'Y'` -- this is because trying to generate synthetic data consistently across multiple records is an order of magnitude more complicated and isn't perceived to yield much additional benefit.\n",
    "3. Discard records having a missing report_date.\n",
    "4. Only retain dates that have at least 1,000 total counts in the raw dataset (here, 2004-2020) -- others not meeting this criterion are considered anomalous (e.g. years 1951, 2109) or not having enough data to reliably make synthetic data (e.g. 2002, 2003).\n",
    "5. For each `report_state` and `condition_code` type, create a zero-padded time series such that the series begins on the first day of the first month of the first event and ends on the last day of the last month of the last event."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import collections\n",
    "import datetime\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "import csv\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start timer\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define which columns will be retained from each dataset out of the possibilities\n",
    "usecols_NETSS = [\n",
    " 'AGE',\n",
    " 'AGETYPE',\n",
    " #'BIRTHD',\n",
    " #'CASEID',\n",
    " 'CASSTAT',\n",
    " #'CDCDATE',\n",
    " 'COUNT',\n",
    " 'COUNTY',\n",
    " #'COUNTY_OF_RESIDENCE',\n",
    " #'DATET',\n",
    " #'DISEASE',\n",
    " 'EVENT',\n",
    " 'EVENTD',\n",
    " #'EVENTN',\n",
    " #'EXPANDED_CASEID',\n",
    " #'EXT_DATE',\n",
    " #'GRAND_TOTAL',\n",
    " 'HISPANIC',\n",
    " #'IMPORT',\n",
    " #'INT_DATE',\n",
    " #'INV501_cd',\n",
    " #'INV501_desc',\n",
    " #'NON_RES_COUNT',\n",
    " #'OTHER',\n",
    " #'OUTBR',\n",
    " #'PRINTED',\n",
    " 'RACE',\n",
    " #'RACECAT',\n",
    " #'RECTYPE',\n",
    " 'SEX',\n",
    " #'SITE',\n",
    " 'STATE',\n",
    " #'STNAME',\n",
    " #'TER_COUNT',\n",
    " #'UPDATE',\n",
    " #'USA_COUNT',\n",
    " #'WEEK',\n",
    " #'WSYSTEM',\n",
    " #'YEAR',\n",
    " #'message_received',\n",
    " #'msg_seq_id',\n",
    " #'msg_transaction_id'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "usecols_HL7 = [\n",
    "'age',\n",
    "'age_units',\n",
    "#'Age_Units_txt',\n",
    "#'Birth_Country',\n",
    "#'Birth_Country_oth',\n",
    "#'Birth_Date',\n",
    "'birth_date_str',\n",
    "#'Case_Status',\n",
    "'case_status_txt',\n",
    "#'Cntry_Usual_Res',\n",
    "#'Cntry_Usual_Res_oth',\n",
    "#'Cntry_Usual_Res_txt',\n",
    "#'Comment',\n",
    "'condition_code',\n",
    "#'Condition_Code_txt',\n",
    "#'Condition_MMG',\n",
    "#'Core_Uid',\n",
    "'diag_dt',\n",
    "#'Diag_Dt_str',\n",
    "#'Died',\n",
    "'died_dt',\n",
    "#'Died_Dt_str',\n",
    "'earliest_cnty_dt',\n",
    "#'Earliest_Cnty_Dt_str',\n",
    "'earliest_state_dt',\n",
    "#'Earliest_State_Dt_str',\n",
    "#'Elect_Notif_CDC_Dt',\n",
    "#'Elect_Notif_CDC_Dt_str',\n",
    "#'Ethnicity',\n",
    "'ethnicity_txt',\n",
    "'first_elec_submit_dt',\n",
    "#'First_Elec_Submit_Dt_str',\n",
    "#'Generic_Version',\n",
    "'hosp_admit_dt',\n",
    "#'Hosp_Admit_Dt_str',\n",
    "#'Hosp_Days',\n",
    "#'Hosp_Dis_Dt',\n",
    "#'Hosp_Dis_Dt_str',\n",
    "#'Hospitalized',\n",
    "#'Illness_Dur',\n",
    "#'Illness_Dur_Units',\n",
    "#'Illness_End_Dt',\n",
    "#'Illness_End_Dt_str',\n",
    "'illness_onset_dt',\n",
    "#'Illness_Onset_Dt_str',\n",
    "#'Immediate_NNC',\n",
    "#'Import_City',\n",
    "#'Import_City_txt',\n",
    "#'Import_Cntry',\n",
    "#'Import_Cntry_oth',\n",
    "#'Import_Cnty',\n",
    "#'Import_Cnty_txt',\n",
    "#'Import_Code',\n",
    "#'Import_Code_txt',\n",
    "#'Import_State',\n",
    "#'Import_State_txt',\n",
    "'invest_start_dt',\n",
    "#'Invest_Start_Dt_str',\n",
    "#'Jurisdiction_Code',\n",
    "#'Legacy_Case_ID',\n",
    "#'Loc_Rec_ID',\n",
    "#'Local_Subj_ID',\n",
    "#'MMWR_Week',\n",
    "#'MMWR_Year',\n",
    "#'MMWR_Year_str',\n",
    "#'Message_Con_ID',\n",
    "#'Message_Dt',\n",
    "#'Message_Dt_str',\n",
    "#'Name_Report_CDC',\n",
    "#'Name_Report_Email',\n",
    "#'Nat_Report_Jurisdiction',\n",
    "'notif_result_status',\n",
    "#'Oth_Birth_Place',\n",
    "#'Oth_Race',\n",
    "#'Outbreak_Ind',\n",
    "#'Outbreak_Name',\n",
    "'phd_notif_dt',\n",
    "#'PHD_Notif_Dt_Str',\n",
    "#'Phone_Report_CDC',\n",
    "'pregnant',\n",
    "#'Proc_ID',\n",
    "#'Profile_Version',\n",
    "'report_county',\n",
    "#'Report_County_txt',\n",
    "'report_dt',\n",
    "#'Report_Dt_str',\n",
    "#'Report_Source_Type',\n",
    "#'Report_Source_Type_oth',\n",
    "#'Report_Source_Zip',\n",
    "'report_state',\n",
    "#'Report_State_txt',\n",
    "#'Report_source_type_txt',\n",
    "#'Sending_App_ID',\n",
    "#'Sending_App_Name',\n",
    "#'Sending_Fac_ID',\n",
    "#'Sending_Fac_Name',\n",
    "'sex',\n",
    "#'State_Case_ID',\n",
    "#'Transmission_Mode',\n",
    "#'Transmission_Mode_oth',\n",
    "#'Verbal_Notif_CDC_Dt',\n",
    "#'Verbal_Notif_CDC_Dt_str',\n",
    "#'birth_country_txt',\n",
    "'current_record_flag',\n",
    "#'import_cntry_txt',\n",
    "#'message_seq_id',\n",
    "#'msg_received_dttm',\n",
    "'msg_transaction_id',\n",
    "#'mvps_datetime_created',\n",
    "#'mvps_datetime_updated',\n",
    "#'nat_report_jurisdiction_txt',\n",
    "#'transmission_mode_txt',\n",
    "# TBD: 'Subj_County'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_map = {'age': 'Age',\n",
    "            'age_units': 'Age_Units',\n",
    "            'birth_date_str': 'Birth_Date_str',\n",
    "            'case_status_txt': 'Case_Status_txt',\n",
    "            'condition_code': 'Condition_Code',\n",
    "            'diag_dt': 'Diag_Dt',\n",
    "            'died_dt': 'Died_Dt',\n",
    "            'earliest_cnty_dt': 'Earliest_Cnty_Dt',\n",
    "            'earliest_state_dt': 'Earliest_State_Dt',\n",
    "            'ethnicity_txt': 'Ethnicity_txt',\n",
    "            'first_elec_submit_dt': 'First_Elec_Submit_Dt',\n",
    "            'hosp_admit_dt': 'Hosp_Admit_Dt',\n",
    "            'illness_onset_dt': 'Illness_Onset_Dt',\n",
    "            'invest_start_dt': 'Invest_Start_Dt',\n",
    "            'notif_result_status': 'Notif_Result_Status',\n",
    "            'phd_notif_dt': 'PHD_Notif_Dt',\n",
    "            'pregnant': 'Pregnant',\n",
    "            'report_county': 'Report_County',\n",
    "            'report_dt': 'Report_Dt',\n",
    "            'report_state': 'Report_State',\n",
    "            'sex': 'Sex'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File path constants\n",
    "JURISDICTION_TO_STATE = '/data/csels/jurisdiction_to_state_mapping.csv'\n",
    "OFFICIAL_FIPS = '/data/csels/us_official_fips_codes.csv'\n",
    "NETSS_OUTPUT_ROOT = '/data/csels/preprocessed_data/netss_update'\n",
    "HL7_OUTPUT_ROOT = '/data/csels/preprocessed_data/hl7_update'\n",
    "NETSS_INPUT = '/data/csels/netss_oct_2020'\n",
    "HL7_INPUT = '/data/csels/hl7_feb_2021'\n",
    "GENERAL_INPUT = '/data/csels'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preferred column output orders and variables\n",
    "netss_output_order = ['EVENTD','COUNT','AGE','AGETYPE','SEX','RACE','HISPANIC','CASSTAT','COUNTY']\n",
    "hl7_output_order = [\n",
    "    'Report_Dt_new','COUNT','Age','Age_Units','Sex','Ethnicity_txt',\n",
    "    'race_mapped','Case_Status_txt', 'Birth_Date_str', \n",
    "    'Notif_Result_Status', 'Pregnant', 'Report_County', \n",
    "    'First_Elec_Submit_Dt','Subj_County', 'Diag_Dt', 'Died_Dt', \n",
    "    'Earliest_Cnty_Dt', 'Earliest_State_Dt', 'Hosp_Admit_Dt',\n",
    "    'Illness_Onset_Dt', 'Invest_Start_Dt', 'PHD_Notif_Dt'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load jurisdiction mappings\n",
    "mapping = pd.read_csv(JURISDICTION_TO_STATE)\n",
    "jurisdiction_code_to_name = dict(zip(mapping['Jurisdiction'], mapping['State']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set of offical fips counties\n",
    "fips = pd.read_csv(OFFICIAL_FIPS, dtype = str)\n",
    "valid_county_set = set(fips['Concept Code'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove all existing preprocessed data\n",
    "This will remove all existing preprocessed data to give a clean slate for this run. It is done to protect against potential contamination that would result should this current run be aborted prematurely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the folders if they exist\n",
    "if os.path.exists(NETSS_OUTPUT_ROOT):\n",
    "    shutil.rmtree(NETSS_OUTPUT_ROOT)\n",
    "    \n",
    "if os.path.exists(HL7_OUTPUT_ROOT):\n",
    "    shutil.rmtree(HL7_OUTPUT_ROOT)\n",
    "\n",
    "# Make new folders\n",
    "os.mkdir(NETSS_OUTPUT_ROOT)\n",
    "os.mkdir(HL7_OUTPUT_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define preprocessing programs\n",
    "NETSS and HL7 data will be processed according to these function definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to apply to netss data\n",
    "# modifies county codes based on conditions\n",
    "def validate_county_code_netss(row):\n",
    "    \n",
    "    #identify row elements\n",
    "    jur_num = row['STATE']\n",
    "    county = row['COUNTY']\n",
    "    \n",
    "    #turn string to int\n",
    "    if isinstance(county, str):\n",
    "        county_num = int(county)\n",
    "    #turn float to int except nan\n",
    "    elif isinstance(county, float):\n",
    "        if np.isnan(county):\n",
    "            county_num = county\n",
    "        else:\n",
    "            county_num = int(county)\n",
    "    else:\n",
    "        county_num = county\n",
    "    #return county if valid and between 0 and 999\n",
    "    #else return 999\n",
    "    if 0 < county_num < 999:\n",
    "        county_try = f'{jur_num:02d}{county_num:03d}'\n",
    "        if county_try in valid_county_set:\n",
    "            return county_num\n",
    "        else:\n",
    "            return 999\n",
    "    else:\n",
    "        return 999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_netss_data(df):\n",
    "    \n",
    "    # Discard records where counts are not explicity set to 1, per CDC recommendations\n",
    "    df = df.loc[df['COUNT'] == 1, :]\n",
    "    \n",
    "    # Discard records with missing EVENTD variables\n",
    "    df = df.loc[np.logical_not(df['EVENTD'].isna()), :]\n",
    "        \n",
    "    # Update from 6/4/2020 - it was noted that EVENTD range from years 1899 to 2201.\n",
    "    # We'll keep only complete years (2014-2019 for this currently being written in 2020). \n",
    "    # This is generalized such that if this script is run in 2021, 2020 will be included, and so on with subsequent years.\n",
    "    valid_year = [x.year in range(2014,int(datetime.date.today().year)) for x in pd.to_datetime(df['EVENTD'], errors = 'coerce')]\n",
    "    df = df.loc[valid_year, :]\n",
    "    \n",
    "    df['COUNTY'] = df.apply(validate_county_code_netss, axis = 1)\n",
    "    \n",
    "    # Obtain jurisdiction combinations\n",
    "    jurisdictions = df['STATE'].unique()\n",
    "    \n",
    "    # Process each jurisdiction\n",
    "    for j in jurisdictions:\n",
    "        \n",
    "        # Get the name of the jurisdiction\n",
    "        jurisdiction_name = jurisdiction_code_to_name[int(j)]\n",
    "        \n",
    "        # Create a folder for this jurisdiction\n",
    "        direc = (NETSS_OUTPUT_ROOT + '/{}').format(jurisdiction_name)\n",
    "        os.mkdir(direc)\n",
    "        \n",
    "        # Subset to current jurisdiction\n",
    "        df_jurisdiction = df.loc[df['STATE'] == j, :]\n",
    "        \n",
    "        # The STATE variable is no longer needed after subsetting\n",
    "        df_jurisdiction = df_jurisdiction.drop(columns=['STATE'])\n",
    "        \n",
    "        # Obtain disease combinations for this jurisdiction\n",
    "        diseases = df_jurisdiction['EVENT'].unique()\n",
    "        \n",
    "        # Process each disease within jurisdiction\n",
    "        for d in diseases:\n",
    "            \n",
    "            # Subset to current disease\n",
    "            df_disease = df_jurisdiction.loc[df['EVENT'] == d, :]\n",
    "            \n",
    "            # The EVENT variable is no longer needed after subsetting\n",
    "            df_disease = df_disease.drop(columns=['EVENT'])\n",
    "            \n",
    "            # Initiate time series dataframe\n",
    "            counts = df_disease.reset_index(drop = True)\n",
    "    \n",
    "            # Capture the time range and make daily increments for the range of data\n",
    "            min_dt = pd.to_datetime(counts['EVENTD']).min()\n",
    "            max_dt = pd.to_datetime(counts['EVENTD']).max()\n",
    "            \n",
    "            # Pad on the left and right to extend the series cleanly to the start/end of months in the data range\n",
    "            series_min_dt = pd.Timestamp('{}-{}-01'.format(min_dt.year,min_dt.month))\n",
    "            series_max_dt = max_dt + pd.tseries.offsets.MonthEnd(0)\n",
    "            all_dates = pd.DataFrame(pd.date_range(start = series_min_dt, end = series_max_dt), columns = ['EVENTD'])\n",
    "            \n",
    "            # Merge the counts to the padded data and set missing counts to 0\n",
    "            counts['EVENTD'] = pd.to_datetime(counts['EVENTD'])\n",
    "            time_series = all_dates.merge(counts, on = ['EVENTD'], how = 'left')\n",
    "            time_series['COUNT'] = time_series['COUNT'].fillna(0)\n",
    "            \n",
    "            # Export this dataset as a csv file within the jurisdiction directory\n",
    "            filename = '{}.csv'.format(d)\n",
    "            filepath = os.path.join(NETSS_OUTPUT_ROOT, jurisdiction_name, filename)\n",
    "            time_series[netss_output_order].to_csv(filepath, index = False)\n",
    "            #time_series[netss_output_order].to_csv('./netss_update/{}/{}.csv'.format(jurisdiction_name, d), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to apply to hl7 data\n",
    "#modifies county codes based on condition\n",
    "def validate_county_code_hl7_rep(row):\n",
    "    \n",
    "    #identify row elements\n",
    "    jur_num = row['Report_State']\n",
    "    county = row['Report_County']\n",
    "    \n",
    "    #try if data is string else return '99999'\n",
    "    try:\n",
    "        #must be five digit string\n",
    "        if re.fullmatch(r'\\d{5}', county):\n",
    "            #county jurisdiction\n",
    "            jur_test = county[:2]\n",
    "            #check if county jurisdiction matches actual\n",
    "            if jur_test == f'{jur_num:02d}':\n",
    "                #keep county if valid\n",
    "                if county in valid_county_set:\n",
    "                    return county\n",
    "                #keep state jurisdiction if match\n",
    "                else:\n",
    "                    return f'{jur_num:02d}999'\n",
    "            #jurisdiction doesn't match\n",
    "            else:\n",
    "                return jur_test + '999'\n",
    "        #replace non five digit string\n",
    "        else:\n",
    "            return '99999'\n",
    "    except:\n",
    "        return '99999'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_county_code_hl7_subj(row):\n",
    "    \n",
    "    #identify row elements\n",
    "    jur_num = row['Report_State']\n",
    "    county = row['Subj_County']\n",
    "    \n",
    "    #try if data is string else return '99999'\n",
    "    try:\n",
    "        #must be five digit string\n",
    "        if re.fullmatch(r'\\d{5}', county):\n",
    "            #county jurisdiction\n",
    "            jur_test = county[:2]\n",
    "            #check if county jurisdiction matches actual\n",
    "            if jur_test == f'{jur_num:02d}':\n",
    "                #keep county if valid\n",
    "                if county in valid_county_set:\n",
    "                    return county\n",
    "                #keep state jurisdiction if match\n",
    "                else:\n",
    "                    return f'{jur_num:02d}999'\n",
    "            #jurisdiction doesn't match\n",
    "            else:\n",
    "                return jur_test + '999'\n",
    "        #replace non five digit string\n",
    "        else:\n",
    "            return '99999'\n",
    "    except:\n",
    "        return '99999'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_hl7_data(df):\n",
    "    \n",
    "    # Exclude rows where current_record_flag is not 'Y'\n",
    "    df = df.loc[df['current_record_flag'] == 'Y', :]\n",
    "    df = df.drop(columns = 'current_record_flag')\n",
    "    \n",
    "    # Exclude rows where report_dt is missing\n",
    "    df = df.copy() # to avoid SettingWithCopyWarning\n",
    "    lookup = df.loc[:, ['Report_Dt', 'PHD_Notif_Dt', 'Earliest_State_Dt', 'Earliest_Cnty_Dt']].notnull().idxmax(1)\n",
    "    idx, cols = pd.factorize(lookup)\n",
    "    df['Report_Dt_new'] = df.reindex(cols, axis = 1).to_numpy()[np.arange(len(df)), idx]\n",
    "    #df = df.loc[np.logical_not(df['Report_Dt_new'].isna()), :]\n",
    "    \n",
    "    # Calculate the total number of observations for each year and exclude if < 1000\n",
    "    report_dt = pd.to_datetime(df['First_Elec_Submit_Dt'])\n",
    "    report_yr = [x.year for x in report_dt]\n",
    "    cnt = collections.Counter(report_yr)\n",
    "    valid_years = [x for x, count in cnt.items() if count >= 1000]\n",
    "    df = df.loc[pd.Series(report_yr).isin(valid_years).values, :]\n",
    "    \n",
    "    # Replace report_dt with the date part of the full string expression\n",
    "    df['First_Elec_Submit_Dt'] = df['First_Elec_Submit_Dt'].str.slice(stop=10)\n",
    "    \n",
    "    # Set COUNT variable to 1 (to align with the NETSS 1 row = 1 COUNT methodology)\n",
    "    df['COUNT'] = 1\n",
    "    \n",
    "    df = df[df['Report_State'].notna()]\n",
    "    df['Report_State'] = df['Report_State'].astype(int)\n",
    "    \n",
    "    df['Report_County'] = df.apply(validate_county_code_hl7_rep, axis = 1)\n",
    "    df['Subj_County'] = df.apply(validate_county_code_hl7_subj, axis = 1)\n",
    "    \n",
    "    # Obtain jurisdiction combinations\n",
    "    jurisdictions = df['Report_State'].unique()\n",
    "    \n",
    "    # Process each jurisdiction\n",
    "    for j in jurisdictions:\n",
    "        \n",
    "        # Get the name of the jurisdiction\n",
    "        jurisdiction_name = jurisdiction_code_to_name[int(j)]\n",
    "        \n",
    "        # Create a folder for this jurisdiction\n",
    "        direc = (HL7_OUTPUT_ROOT + '/{}').format(jurisdiction_name)\n",
    "        os.mkdir(direc)\n",
    "        \n",
    "        # Subset to current jurisdiction\n",
    "        df_jurisdiction = df.loc[df['Report_State'] == j, :]\n",
    "        \n",
    "        # The report_state variable is no longer needed after subsetting\n",
    "        df_jurisdiction = df_jurisdiction.drop(columns=['Report_State'])\n",
    "        \n",
    "        # Obtain disease combinations for this jurisdiction\n",
    "        diseases = df_jurisdiction['Condition_Code'].unique()\n",
    "        \n",
    "        # Process each disease within jurisdiction\n",
    "        for d in diseases:\n",
    "            \n",
    "            # Subset to current disease\n",
    "            df_disease = df_jurisdiction.loc[df['Condition_Code'] == d, :]\n",
    "            \n",
    "            # The condition_code variable is no longer needed after subsetting\n",
    "            df_disease = df_disease.drop(columns=['Condition_Code'])\n",
    "            \n",
    "            # Initiate time series dataframe\n",
    "            counts = df_disease.reset_index(drop = True)\n",
    "    \n",
    "            # Capture the time range and make daily increments for the range of data\n",
    "            min_dt = pd.to_datetime(counts['First_Elec_Submit_Dt']).min()\n",
    "            max_dt = pd.to_datetime(counts['First_Elec_Submit_Dt']).max()\n",
    "            \n",
    "            # Pad on the left and right to extend the series cleanly to the start/end of months in the data range\n",
    "            series_min_dt = pd.Timestamp('{}-{}-01'.format(min_dt.year,min_dt.month))\n",
    "            series_max_dt = max_dt + pd.tseries.offsets.MonthEnd(0)\n",
    "            all_dates = pd.DataFrame(pd.date_range(start = series_min_dt, end = series_max_dt), \n",
    "                                     columns = ['First_Elec_Submit_Dt'])\n",
    "            \n",
    "            # Merge the counts to the padded data and set missing counts to 0\n",
    "            counts['First_Elec_Submit_Dt'] = pd.to_datetime(counts['First_Elec_Submit_Dt'])\n",
    "            time_series = all_dates.merge(counts, on = ['First_Elec_Submit_Dt'], how = 'left')\n",
    "            time_series['COUNT'] = time_series['COUNT'].fillna(0)\n",
    "            \n",
    "            # Export this dataset as a csv file within the jurisdiction directory\n",
    "            filename = '{}.csv'.format(d)\n",
    "            filepath = os.path.join(HL7_OUTPUT_ROOT, jurisdiction_name, filename)\n",
    "            time_series[hl7_output_order].to_csv(filepath, index = False)\n",
    "            #time_series[hl7_output_order].to_csv('./hl7_update/{}/{}.csv'.format(jurisdiction_name, d), index = False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date parsing utility function for the function to follow\n",
    "def dateParse(df, x):\n",
    "    \n",
    "    # It seems that illness_end_dt_str are formatted as YYYYMMDD, while other datetime variables are YYYY-MM-DD.\n",
    "    # However, there are other peculiaries, such as illness_end_dt_str ending in .000 and datetime variables\n",
    "    # requiring their time component to be stripped away, which is hanlded by the string slicing below.\n",
    "    if x == 'Illness_End_Dt_str':\n",
    "        return pd.to_datetime(df[x].str.slice(start = 0, stop = 8), errors='coerce', format = '%Y%m%d')\n",
    "    else:\n",
    "        return pd.to_datetime(df[x].str.slice(start = 0, stop = 10), errors='coerce', format = '%Y-%m-%d')\n",
    "\n",
    "# Function to take nonmissing dates in reference to their days from Report_Dt, store as a tuple, and count unique values    \n",
    "def create_tuple_csv(df):\n",
    "    \n",
    "    # Exclude rows where report_dt is missing\n",
    "    df = df.copy() # to avoid SettingWithCopyWarning\n",
    "    lookup = df.loc[:, ['Report_Dt', 'PHD_Notif_Dt', 'Earliest_State_Dt', 'Earliest_Cnty_Dt']].notnull().idxmax(1)\n",
    "    idx, cols = pd.factorize(lookup)\n",
    "    df['Report_Dt_new'] = df.reindex(cols, axis = 1).to_numpy()[np.arange(len(df)), idx]\n",
    "    #df = df.loc[np.logical_not(df['Report_Dt_new'].isna()), :]\n",
    "    \n",
    "    # Necessary variables\n",
    "    core_dts = df[['Diag_Dt', 'Died_Dt', 'First_Elec_Submit_Dt', 'Hosp_Admit_Dt', 'Illness_Onset_Dt', 'Invest_Start_Dt',\n",
    "                  'Report_Dt_new', 'current_record_flag']]\n",
    "    \n",
    "    # Apply restrictions that we intend to apply before generating synthetic data\n",
    "\n",
    "    # Exclude rows where current_record_flag is not 'Y'\n",
    "    core_dts = core_dts.loc[core_dts['current_record_flag'] == 'Y', :]\n",
    "    core_dts = core_dts.drop(columns = 'current_record_flag')\n",
    "\n",
    "    # Exclude rows where report_dt is missing\n",
    "    core_dts = core_dts.copy() # to avoid SettingWithCopyWarning\n",
    "    #core_dts = core_dts.loc[np.logical_not(core_dts['Report_Dt_new'].isna()), :]\n",
    "    \n",
    "    # Parse the dates to string format\n",
    "    for i in ['Diag_Dt', 'Died_Dt', 'First_Elec_Submit_Dt', 'Hosp_Admit_Dt', 'Illness_Onset_Dt', 'Invest_Start_Dt', \n",
    "              'Report_Dt_new']:\n",
    "        core_dts[i] = dateParse(core_dts, i)\n",
    "    \n",
    "    # List of each individual date difference tuple \n",
    "    # Only tuples where all values are nonnegative are kept\n",
    "    tupes = []\n",
    "    for index, row in core_dts.iterrows():\n",
    "        report = row['First_Elec_Submit_Dt']\n",
    "        if row['Diag_Dt'] != 'None': \n",
    "            a = (row['Diag_Dt'] - report) / np.timedelta64(1, 'D')\n",
    "        else: \n",
    "            a = 'None'\n",
    "        if row['Died_Dt'] != 'None': \n",
    "            b = (row['Died_Dt'] - report) / np.timedelta64(1, 'D') \n",
    "        else: \n",
    "            b = 'None'\n",
    "        if row['Report_Dt_new'] != 'None': \n",
    "            c = (row['Report_Dt_new'] - report) / np.timedelta64(1, 'D') \n",
    "        else: \n",
    "            c = 'None'\n",
    "        if row['Hosp_Admit_Dt'] != 'None': \n",
    "            d = (row['Hosp_Admit_Dt'] - report) / np.timedelta64(1, 'D') \n",
    "        else: \n",
    "            d = 'None'\n",
    "        if row['Illness_Onset_Dt'] != 'None': \n",
    "            e = (row['Illness_Onset_Dt'] - report) / np.timedelta64(1, 'D') \n",
    "        else: \n",
    "            e = 'None'\n",
    "        if row['Invest_Start_Dt'] != 'None': \n",
    "            f = (row['Invest_Start_Dt'] - report) / np.timedelta64(1, 'D') \n",
    "        else: \n",
    "            f = 'None'\n",
    "        tupes.append((a,b,c,d,e,f))\n",
    "    \n",
    "    # Counter object for each individual tuple           \n",
    "    counts = collections.Counter(tupes)\n",
    "    hist = pd.DataFrame.from_dict(counts, orient='index').reset_index()\n",
    "    hist[['d1','d2','d3','d4','d5','d6']] = pd.DataFrame(hist['index'].tolist(), index=hist.index)\n",
    "    hist = hist.drop('index', axis = 1)\n",
    "    hist = hist.rename(columns = {0: 'count'})\n",
    "    \n",
    "    # Csv file writer\n",
    "    filename = 'tuple_histogram.csv'\n",
    "    filepath = os.path.join(HL7_OUTPUT_ROOT, filename)\n",
    "    hist.to_csv(filepath)\n",
    "    #hist.to_csv('./hl7_update/tuple_histogram.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define a utility function to transform the `gen_race.csv` file into a condensed (1 row = 1 `msg_transaction_id`) format. This allows it to be joined to core, which adds a single column called `race_mapped`. This column is a concatenation of all races given by the corresponding `msg_transaction_id` in the race table.\n",
    "\n",
    "Please refer to this reference when considering the race code mappings:\n",
    "https://phinvads.cdc.gov/vads/ViewValueSet.action?oid=2.16.840.1.114222.4.11.7205"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_condensed_race_table():\n",
    "    \n",
    "    # Load race table\n",
    "    filename = 'genRace.csv'\n",
    "    filepath = os.path.join(HL7_INPUT, filename)\n",
    "    race = pd.read_csv(filepath, dtype = str, encoding = 'cp1252', quotechar = '^')\n",
    "    #race = pd.read_csv('../hl7_feb_2021/genRace.csv',\n",
    "    #                   dtype = str, encoding = 'cp1252', quotechar = '^')\n",
    "\n",
    "    \n",
    "    # Define race code mapping to get codes aligned with standard concepts\n",
    "    race_code_map = {\n",
    "        '2106-3':'2106-3',               # White\n",
    "        'UNK':'NullFlavor',              # NullFlavor (Unknown)\n",
    "        '2054-5':'2054-5',               # Black or African American\n",
    "        '2131-1':'2131-1',               # Other Race\n",
    "        '2028-9':'2028-9',               # Asian\n",
    "        '1002-5':'1002-5',               # American Indian or Alaska Native\n",
    "        '2076-8':'2076-8',               # Native Hawaiian or Other Pacific Islander\n",
    "        'C':'NullFlavor',                # NullFlavor (Unknown)\n",
    "        'PHC1175':'NullFlavor',          # NullFlavor (Refused to answer)\n",
    "        'U':'NullFlavor',                # NullFlavor (Unknown)\n",
    "        'NASK':'NullFlavor',             # NullFlavor (Not Asked)\n",
    "        'H':'NullFlavor',                # NullFlavor (Unknown)\n",
    "        'M':'NullFlavor',                # NullFlavor (Unknown)\n",
    "        'White':'2106-3',                # White\n",
    "        'X':'NullFlavor',                # NullFlavor (Unknown)\n",
    "        'R':'NullFlavor',                # NullFlavor (Unknown)\n",
    "        '2054-4':'2054-5',               # Black or African American (Presumed inteded to be 2054-5)\n",
    "        'WHITE (CAUCASIAN)':'2106-3',    # White\n",
    "        'WH':'2106-3',                   # White\n",
    "        'U':'NullFlavor',                # NullFlavor (Presumed Unknown)\n",
    "        'AI':'1002-5',                   # Presumed AI stands for American Indian\n",
    "        'NH':'2076-8',                   # Presumed NH stands for Native Hawaiian\n",
    "        'NOT PROVIDED':'NullFlavor',     # NullFlavor\n",
    "        'UNKNOWN':'NullFlavor',          # NullFlavor\n",
    "        'Unknown':'NullFlavor',          # NullFlavor\n",
    "        'PHC1367':'NullFlavor',          # NullFlavor (Refused)\n",
    "        'P':'NullFlavor',                # NullFlavor (Unknown)\n",
    "        'Pt Declined':'NullFlavor',      # NullFlavor\n",
    "        'CAUCASIAN':'2106-3',            # White\n",
    "        'CA':'2106-3',                   # White\n",
    "        'Black':'2054-5',                # Black or African American\n",
    "        'N':'NullFlavor',                # NullFlavor\n",
    "        'Not Reported':'NullFlavor',     # NullFlavor\n",
    "        '2058-6':'2131-1',               # Other (Undocumented code)\n",
    "        'OTHER NONWHITE':'2131-1',       # Other\n",
    "        'Black or African Ame':'2054-5', # Black or African American\n",
    "        '1004-1':'1002-5',               # American Indian\n",
    "        'ASKU':'NullFlavor',             # NullFlavor (Asked but unknown)\n",
    "        '1':'NullFlavor',                # NullFlavor (Unknown)\n",
    "        '2186-5':'2131-1',               # Other (Undocumented code)\n",
    "        '2076-8s':'2131-1',              # Other (Undocumented code)\n",
    "        'code failure':'NullFlavor',     # NullFlavor\n",
    "        'AFRICAN AMERICAN OR':'2054-5',  # Black or African American\n",
    "        'Other Race':'2131-1',           # Other\n",
    "        'Hispanic':'2131-1',             # Other\n",
    "        'Hispanic or Latino':'2131-1',   # Other\n",
    "        'FALSE':'NullFlavor',            # NullFlavor\n",
    "        'ASIAN':'2028-9',                 # Asian\n",
    "        #new\n",
    "        '1002-5, 2054-5, 2106': '2131-1',\n",
    "        '2029-7': '2028-9',\n",
    "        '2036-2': '2028-9',\n",
    "        '2039-6': '2028-9',\n",
    "        '2086-7': '2028-9',\n",
    "        'AA': '2054-5',\n",
    "        'AS': '2028-9',\n",
    "        'Asian': '2028-9',\n",
    "        'CAC': 'NullFlavor',\n",
    "        'CACH': 'NullFlavor',\n",
    "        'CAF': 'NullFlavor',\n",
    "        'CAG': 'NullFlavor',\n",
    "        'CAH': 'NullFlavor',  \n",
    "        'CAI': 'NullFlavor',\n",
    "        'CAJ': 'NullFlavor',\n",
    "        'CAK': 'NullFlavor',\n",
    "        'CAL': 'NullFlavor',\n",
    "        'CAM': 'NullFlavor',\n",
    "        'CAMI': 'NullFlavor',\n",
    "        'CAMP': 'NullFlavor',\n",
    "        'CAN': 'NullFlavor',\n",
    "        'CAO': 'NullFlavor',\n",
    "        'CAOT': 'NullFlavor',\n",
    "        'CAP': 'NullFlavor',\n",
    "        'CAS': 'NullFlavor',\n",
    "        'CAV': 'NullFlavor',\n",
    "        'Caucasian': '2106-3',\n",
    "        'FALSE': 'NullFlavor',\n",
    "        'FEMALE': 'NullFlavor',\n",
    "        'O': '2131-1',\n",
    "        'OTHER ASIAN': '2131-1',\n",
    "        'Other': '2131-1',\n",
    "        'PHC1367': 'NullFlavor',\n",
    "        'S': 'NullFlavor',\n",
    "        'WHITE': '2016-3',\n",
    "        ' ': 'NullFlavor'\n",
    "    }\n",
    "\n",
    "    # Apply mapping\n",
    "    race['race_mapped'] = race['race'].map(race_code_map)\n",
    "\n",
    "    # Calculate condensed race by group\n",
    "    g = race.groupby('msg_transaction_id')['race_mapped']\n",
    "    race_condensed = g.apply(lambda x: ';'.join(sorted(x.unique()))).reset_index()\n",
    "    \n",
    "    # A subsequent step is required as an artifact of the NullFlavor mappings\n",
    "    # Since an individual can be marked, for example, as both \"unknown\" and \"refused\", they can have two or more NullFlavor entries when condensed\n",
    "    # We will map such cases to a single NullFlavor entry.\n",
    "    is_nullflavor = race_condensed['race_mapped'].map(lambda x: set(x.split(';')) == {'NullFlavor'})\n",
    "    race_condensed.loc[is_nullflavor, 'race_mapped'] = 'NullFlavor'\n",
    "    \n",
    "    # We will also condense string sequences of the form x1;x2;...;xn;NullFlavor to be x1;x2;...;xn\n",
    "    # If at least one race is indicated, then we posit that a NullFlavor suffix is not necessary\n",
    "    race_condensed['race_mapped'] = race_condensed['race_mapped'].map(lambda x: ';'.join([item for item in x.split(';') if item not in 'NullFlavor']) if ((len(x.split(';')) > 1) and ('NullFlavor' in x.split(';'))) else x)\n",
    "\n",
    "    # This dataset is now ready to be joined to core\n",
    "    return race_condensed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load, preview, and process data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NETSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = os.path.join(NETSS_INPUT, 'year2015a.csv')\n",
    "file2 = os.path.join(NETSS_INPUT, 'year2016a.csv')\n",
    "file3 = os.path.join(NETSS_INPUT, 'year2017pt1.csv')\n",
    "file4 = os.path.join(NETSS_INPUT, 'year2017pt2a.csv')\n",
    "file5 = os.path.join(NETSS_INPUT, 'year2018.csv')\n",
    "file6 = os.path.join(NETSS_INPUT, 'nndss19.csv')\n",
    "file7 = os.path.join(GENERAL_INPUT, 'nndss20_update.csv')\n",
    "netss1 = pd.read_csv(file1, usecols = usecols_NETSS,\n",
    "                    dtype = {'EVENTD':str,'COUNT':int,'STATE':int,'EVENT':int})\n",
    "netss2 = pd.read_csv(file2, usecols = usecols_NETSS,\n",
    "                    dtype = {'EVENTD':str,'COUNT':int,'STATE':int,'EVENT':int})\n",
    "netss3 = pd.read_csv(file3, usecols = usecols_NETSS,\n",
    "                    dtype = {'EVENTD':str,'COUNT':int,'STATE':int,'EVENT':int})\n",
    "netss4 = pd.read_csv(file4, usecols = usecols_NETSS,\n",
    "                    dtype = {'EVENTD':str,'COUNT':int,'STATE':int,'EVENT':int})\n",
    "netss5 = pd.read_csv(file5, usecols = usecols_NETSS,\n",
    "                    dtype = {'EVENTD':str,'COUNT':int,'STATE':int,'EVENT':int})\n",
    "netss6= pd.read_csv(file6, usecols = usecols_NETSS,\n",
    "                    dtype = {'EVENTD':str,'COUNT':int,'STATE':int,'EVENT':int})\n",
    "netss7 = pd.read_csv(file7, usecols = usecols_NETSS,\n",
    "                    dtype = {'EVENTD':str,'COUNT':int,'STATE':int,'EVENT':int})\n",
    "netss = pd.concat([netss1,netss2,netss3,netss4,netss5,netss6,netss7], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview NETSS data head (Optional)\n",
    "# netss.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview NETSS data tail (Optional)\n",
    "# netss.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process netss data\n",
    "preprocess_netss_data(netss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HL7 - gen_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data subset to relevant columns for processing\n",
    "#hl7 = pd.read_csv('../hl7_feb_2021/genCore_Export_y2.zip', usecols = usecols_HL7,\n",
    "                  #quotechar='^', dtype = str, header = (0), encoding = 'cp1252', compression = 'zip')\n",
    "file1 = os.path.join(HL7_INPUT, 'genCore_2019_Export_transid.csv')\n",
    "file2 = os.path.join(HL7_INPUT, 'genCore_2020_Export_transid.csv')\n",
    "hl719 = pd.read_csv(file1, header = (1), quotechar = '^', dtype = str, \n",
    "                    encoding = 'cp1252', usecols = usecols_HL7)\n",
    "hl720 = pd.read_csv(file2, quotechar = '^', dtype = str, encoding = 'cp1252',\n",
    "                    usecols = usecols_HL7)\n",
    "hl7 = pd.concat([hl719,hl720], ignore_index = True)\n",
    "hl7 = hl7.rename(mapper = name_map, axis = 1)\n",
    "#hl7 = pd.read_csv('../hl7_oct_2020/GenCore.csv',\n",
    "#                    usecols = usecols_HL7,\n",
    "#                    dtype = str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the condensed race table -- If this fails with a TypeError, it implies that not all of the race categories were accounted for\n",
    "try:\n",
    "    race_condensed = make_condensed_race_table()\n",
    "except TypeError:\n",
    "    print(\"Please ensure that 'race_code_map' contains all of the mappings below:\")\n",
    "    file = os.path.join(HL7_INPUT, 'genRace.csv')\n",
    "    df_race = pd.read_csv(file, usecols = ['race'], dtype = str, encoding = 'cp1252', quotechar = '^')\n",
    "    for x in sorted(df_race['race'].unique()):\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join this to core\n",
    "hl7 = hl7.merge(race_condensed, on = 'msg_transaction_id', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in file to merge with hl7\n",
    "# Only need one variable\n",
    "file = os.path.join(HL7_INPUT, 'genAddress.csv')\n",
    "subj_county = pd.read_csv(file, \n",
    "usecols = ['msg_transaction_id', 'Subj_County'], \n",
    "dtype = str, encoding = 'cp1252', quotechar = '^').dropna().reset_index(drop = True)\n",
    "\n",
    "# Joining the variable to the core\n",
    "hl7 = hl7.merge(subj_county, on = 'msg_transaction_id', how = 'left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# A subsequent step here is required to address cases where the msg_transaction_id in core was absent from the race table.\n",
    "# Such cases will be mapped to NullFlavor.\n",
    "hl7.loc[hl7['race_mapped'].isna(),'race_mapped'] = 'NullFlavor'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview HL7 data head (Optional)\n",
    "# hl7.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview HL7 data tail (Optional)\n",
    "# hl7.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process HL7 data\n",
    "preprocess_hl7_data(hl7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tuple histogram csv\n",
    "#create_tuple_csv(hl7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed:  2.8 Hours\n"
     ]
    }
   ],
   "source": [
    "# End timer\n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"Time elapsed: \", round(elapsed_time/3600,2), \"Hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
